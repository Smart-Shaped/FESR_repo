services:
  namenode:
    image: apache/hadoop:3.3.6
    build:
      context: ./hadoop_image
      dockerfile: Dockerfile
    hostname: namenode
    command: ["hdfs", "namenode"]
    ports:
      - 9870:9870
    env_file:
      - ./hadoop_image/hadoop.env
    environment:
      ENSURE_NAMENODE_DIR: "/opt/hadoop/dfs/name"
    volumes:
      - hadoop_namenode:/opt/hadoop/dfs/name
      - ./spark_jars:/opt/hadoop/dfs/spark/jars

  datanode:
    image: apache/hadoop:3.3.6
    build:
      context: ./hadoop_image
      dockerfile: Dockerfile
    command: ["hdfs", "datanode"]
    env_file:
      - ./hadoop_image/hadoop.env    
    depends_on:
      - namenode
    volumes:
      - hadoop_datanode:/opt/hadoop/dfs/data

  resourcemanager:
    image: apache/hadoop:3.3.6
    build:
      context: ./hadoop_image
      dockerfile: Dockerfile
    hostname: resourcemanager
    command: ["yarn", "resourcemanager"]
    ports:
      - 8088:8088
    depends_on:
      - namenode
      - datanode
    env_file:
      - ./hadoop_image/hadoop.env

  nodemanager:
    image: apache/hadoop:3.3.6
    build:
      context: ./hadoop_image
      dockerfile: Dockerfile
    command: ["yarn", "nodemanager"]
    ports:
      - 8042:8042
    depends_on:
      - namenode
      - datanode
      - resourcemanager
    env_file:
      - ./hadoop_image/hadoop.env

  spark-master:
    build:
      dockerfile: ./spark_image/Dockerfile
    environment:
      - SPARK_MODE=master
      - SPARK_USER=spark
      - SPARK_CONF_DIR=/opt/bitnami/spark/conf
      - APP_ENV=local
    ports:
      - "8080:8080"
      - "4040:4040"
      - "18080:18080"
    volumes:
      - ./jars:/opt/bitnami/spark/extra_jars
      - ./conf:/opt/bitnami/spark/conf
      - ./hadoop:/opt/bitnami/hadoop
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    runtime: nvidia
    shm_size: 1g
    ulimits:
      memlock:
        soft: -1
        hard: -1
      stack:
        soft: 67108864
        hard: 67108864
    stdin_open: true
    tty: true
    pull_policy: always

  spark-worker:
    build:
      dockerfile: ./spark_image/Dockerfile
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=20G
      - SPARK_WORKER_CORES=4
      - SPARK_USER=spark
      - SPARK_CONF_DIR=/opt/bitnami/spark/conf
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    volumes:
      - ./conf:/opt/bitnami/spark/conf
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    runtime: nvidia
    shm_size: 1g
    ulimits:
      memlock:
        soft: -1
        hard: -1
      stack:
        soft: 67108864
        hard: 67108864
    stdin_open: true
    tty: true
    pull_policy: always

  cassandra:
    image: cassandra:latest
    hostname: cassandra1
    ports:
      - "9042:9042"
    volumes:
      - cassandra_data:/var/lib/cassandra
    environment:
      - CASSANDRA_CLUSTER_NAME=MyCluster

  prometheus:
    image: prom/prometheus
    hostname: prometheus1
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml

  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana-storage:/var/lib/grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    depends_on:
      - prometheus

  loki:
    image: grafana/loki:latest
    ports:
      - "3100:3100"

volumes:
  hadoop_namenode:
  hadoop_datanode:
  cassandra_data:
  grafana-storage:
